\chapter{Background/Related Work}
%What do I need to know to understand the approach described in the methodology section?

\section{Linguistic Complexity}
%-What is complexity? How it is defined? in SLA literature?
%-Distiction between complexity, accuracy, and fluency (CAF Framework).
%- Types of Complexity: Syntactic, lexical, morphological, discourse?
%- Why complexity is a useful proxy for proficiency. (motivate what is being measured)
%- Mention Task effect?

% i need to be more explicit in differentiating complexity from criterial features - as to some it may seem like they
% overlap. Frequency measures used
% in
% Complexity look at elaboration whereas the frequency in criterial features focuses on the appearance at different
% levels as a developmental pattern.

In second language acquisition (SLA) research a learner's proficiency is analyzed through the triad of domains known
as \textbf{complexity}, \textbf{accuracy}, and \textbf{fluency} (CAF) \cite{Skehan1989,ellis2003}. Each of these
dimensions are treated as key indicators of language development in a learner, with each dimension capturing different
aspects of learner performance. \textbf{Accuracy} is concerned with the extent a learner's language conforms with
the norms of the target language, usually measured by an error rate. While this seems clear-cut, there is
controversy regarding the criteria for evaluating and identifying errors and how much they should follow
prescriptive norms of the target language \cite{housen2009}. \textbf{Fluency} focuses on the learner's
speed and smoothness of production, measured by counting the rate of production units. Due to the nature of data
needing a time variable, usually oral data is analyzed for these studies. Finally,
\textbf{complexity} is characterized as the ”the extent to which the language
produced in performing a task is elaborate
and varied" \cite{ellis2003}. Complexity has attracted considerable attention
in studies analysing written language as a proxy for development.

Despite its widespread use, the concept of complexity has been inconsistently defined across studies. The
definitions of complexity vary across research depending on whether form-based, psycholnguistic, or computational
perspectives are prioritized. To address
this,researchers have introduced more refined distinctions such as, 'absolute vs relative' complexity
\citet{Miestamo2008, Butle2012}, and
'linguistic' vs. 'cognitive' complexity \cite{housen2009}. In these models, the first term refers to characteristics
of the linguistic system itself (e.g. structural elaboration), including how complex a sentence is in terms of its
syntax (e.g. number of clauses, subordination, embedding) or its vocabulary (e.g., lexical diversity, word frequency)
. These features can be measured objectively from the surface structure of the texts independent of the learner.
Whereas the second terms (relative and cognitive) focused on the processing demand on the learner or reader.
These distinctions attempt to separate complexity as a
property of the language system from the mental effort required to process it.

Further clarifying this divide, \citet{Pallotti2015} argues for distinguishing between complexity, as the formal
characteristics of linguistic structures, and \textbf{difficulty}, which refers to the cognitive demands those
structures place on learners and readers. According to this view, a sentence may be structurally complext but not
necessarily difficult for a proficient speaker, while a relatively simple structure could still pose challenges for
a learner.  This distinction is especially important when analyzing learner texts and modeling readability where
both the linguistic form and the processing load matter.


% contrast readability studies with writing studies and then introduce how I plan to approach complexity definition
In line with this perspective, the present study adopts a dual approach, considering both formal complexity and
difficulty measures as dimensions of learner language. While structural complexity is quantified through
surface-level linguistc features, difficulty is inferred from how these features are likely to affect processing and
comprehension. For instance, in readability research, studies studies such as \citet{Berendes2018}, used a range of
global and local complexity measures to classify german secondary school texts for readability, while
\citet{shain2016}employed syntactic complexity measures using Dependency Locality Theory (DLT) to predict reading
times as a measure of cognitive load. Similarly, \citet{Feng2009} analyzed discourse level features related to entity density in texts to assess the readability for adults with intellectual disabilities.

In contrast, most SLA studies on learner writing have focused primarily on linguistic complexity as a developmental
marker, using various syntactic and lexical measures to model language growth
\cite{Lu2010,Lu2011,Vyatkina2012,weiss2019,Iwashita2006,Wolfe1998, Ortega2003,NorrisOrtega2009}. However,
incorporating the dimension of difficulty-particularly in terms of how certain structures may impact
readability or comprehension- offers a more comprehensive understanding of learner proficency. Building on these
distinctions, this
thesis aims to model linguistic complexity within learner langauge that draws from both
structural (syntactic, lexical) and processing-based (cognitive) perspectives in order to judge which measures may
prove fruitful in analyzing L2 Japanese development across learners.

%- Introduce major complexity metrics:
%    -Syntactic: sentence length, clause density, subordination, coordination, noun phrases, dependency distance
%    - Lexical: type-token ratio (TTR), MTLD, lexical sophistication
%-Theoretical basis and interpretation of each measure and limitations
%- Use of NLP tools in measuring complexity and automated tools in ICALL applications (CTAP)

\subsection{Syntactic Complexity}
- General overview on types of measures
    - Elaboration: Global and Local Measures
    - Variation : Diversity of forms and structures
- introduce measures, especially ones I plan to use information on the calcultions should be given in chapter 3 or
in appendix?

- both absolute and relative measures included
    - mean clause length
    - mean sent length
    - clauses to sentences ratio (sentence complexity ratio)
    - Complex NP per sentence
    - NP per sentence
    - coordination
    - subordination
    - adjective and adverb verb modifiers per VP?
    - ADD - average dependency distance?

Syntactic complexity is concerted with quantifying how elaborate and varied the structures used in text are. A text
containing more elaboration usually contains more parts, and consequently longer, therefore length measures are the
standard measures used.


\subsection{Lexical Complexity}
- General overview on what is being measured and motivation behind it
    -difference between lexical words and words
- introduce measures MLTD, CTTR , also mention limitations?
- lexical frequency measures different from syntactic complexity in that they don't quantify elaborateness (relative
complexity measure)?

    - Mean word length in syllables? (via hiragana reading and not kanji)
    - Max word length?
    - MTLD (exclude punctuation)
    - CTTR
    - CTTR for each specific pos (adverbs, adjectives, nouns, verbs, )
    - lexical density of different pos
relative lexical complexity (language use)
    - Average frequency in frequency data-base (JLPT lists)

\subsection{Morphological Complexity}
    - MCI-10 and MCI-5 for nouns verbs and adjectives
    - inflection features, (past tense, passive, causitive)
    - Derivation features (nominalization of type X per token using 可能-性 etc. also Adjectives with 的？)

\subsection{Discourse?Semantic?}
Limitations to trying to use measures no available wordbank, etc for Japanese making it difficult to do semantic
complexity measures.

\subsection{Linguistic Complexity in SLA}
-Syntactic complexity and lexical measures as indices for proficiency levels of other languages. (Language Developmental Trajectory)
-Use of complexity in SLA studies, readability studies, and ICALL
-role of learner corpora in operationalizing and tracking complexity.
-research on non-English languages
-Existing studies on complexity in L2 Japanese - the limitations

%limitations of independent variables influencing a study such as task conditions, ..... better placed in the SLA
%section?
Studies on Learner Japanese


\citet{komori2019} - Examined MDD and MHD measures in L2 Japanese - found MHD to be a better index between levels,
however only 2 proficiency levels plus small native group was compared
\citet{Iwashita2006} - used syntactic complexity measures on oral corpus.


\section{Criterial Features}
- Define criteria features: What are they and how they relate to CEFR levels.
- Illustrative descriptors vs. criterial features. (EGP)
- Frequency-based indicators of syntactic sophistication? \cite{Ellis2004}
    - comparison to developmental sequences?
- Examples of criterial features in English (and possible other languages) also describe this in relevance to Japanese
- automated extraction - POLKE


\subsection{Proficiency Assessment Frameworks}
- Overview of CEFR levels ( Is it really necessary to include ACFL?)
- The structure and goals of JLPT (grammar/lexicon targets at each level)
- Historical development of JLPT and including of CEFER-style descriptors from 2012.
- limitations of JLPT, or critiques on standardized tests.
Not sure if this warrants it's own section. Describe proficency assesment measures CEFER, etc. also introduce JLPT
Can-do statements- forms teste


maybe also introduce some skepticism of the testing methods of the test?
Talk about previous work with EGP proficiency classification?
Introduce can-do statements at each of the levels and also summarize the equivalent test used in the corpus?


\section{Automatic Proficency Assesment}
- motivation - describe why automatino is needed
- Overview of existing systems using complexity/criterial features for proficency classification.
- Describe challenges
    - Learner errors impacting feature extraction (decreases robustness)
    -Inconsistent tagging/parsing in morphologically rich or low-resource languages
        -try to find something for Japanese to support some inconsistentices I found when tokenizing sample text
- Describe the aboves' relevance to Japanese, and to this study


