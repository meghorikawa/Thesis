\chapter{Background/Related Work}
%What do I need to know to understand the approach described in the methodology section?

\section{Linguistic Complexity}
%-What is complexity? How it is defined? in SLA literature?
%-Distiction between complexity, accuracy, and fluency (CAF Framework).
%- Types of Complexity: Syntactic, lexical, morphological, discourse?
%- Why complexity is a useful proxy for proficiency. (motivate what is being measured)
%- Mention Task effect?

% i need to be more explicit in differentiating complexity from criterial features - as to some it may seem like they
% overlap. Frequency measures used
% in
% Complexity look at elaboration whereas the frequency in criterial features focuses on the appearance at different
% levels as a developmental pattern.

In second language acquisition (SLA) research a learner's proficiency is analyzed through the triad of domains known
as \textbf{complexity}, \textbf{accuracy}, and \textbf{fluency} (CAF) \cite{Skehan1989,ellis2003}. Each of these
dimensions are treated as key indicators of language development in a learner, with each dimension capturing different
aspects of learner performance. \textbf{Accuracy} is concerned with the extent a learner's language conforms with
the norms of the target language, usually measured by an error rate. While this seems clear-cut, there is
controversy regarding the criteria for evaluating and identifying errors and how much they should follow
prescriptive norms of the target language \cite{housen2009}. \textbf{Fluency} focuses on the learner's
speed and smoothness of production, measured by counting the rate of production units. Due to the nature of data
needing a time variable, usually oral data is analyzed for these studies. Finally,
\textbf{complexity} is characterized as the ”the extent to which the language
produced in performing a task is elaborate
and varied" \cite{ellis2003}. Complexity has attracted considerable attention
in studies analysing written language as a proxy for development.

Despite its widespread use, the concept of complexity has been inconsistently defined across studies. Researchers
have introduced more refined distinctions such as, 'absolute vs relative'
complexity
\citet{Miestamo2008, Butle2012}, and
'linguistic' vs. 'cognitive' complexity \cite{housen2009}. In these models, the first term refers to characteristics
of the linguistic system itself (e.g. structural elaboration), including how complex a sentence is in terms of its
syntax (e.g. number of clauses, subordination, embedding) or its vocabulary (e.g., lexical diversity, word frequency)
. These features can be measured objectively from the surface structure of the texts independent of the learner.
Whereas the second terms (relative and cognitive) focused on the processing demand on the learner or reader.
These distinctions attempt to separate complexity as a
property of the language system from the mental effort required to process it.

Further clarifying this divide, \citet{Pallotti2015} argues for distinguishing between complexity, as the formal
characteristics of linguistic structures, and \textbf{difficulty}, which refers to the cognitive demands those
structures place on learners and readers. According to this view, a sentence may be structurally complext but not
necessarily difficult for a proficient speaker, while a relatively simple structure could still pose challenges for
a learner.

 In line with this perspective, this study adopts a dual approach, considering both formal complexity and
difficulty measures as dimensions of learner language. While structural complexity is quantified through
surface-level linguistc features, difficulty is inferred from how these features are likely to affect processing and
comprehension. For instance, in readability research, studies studies such as \citet{Berendes2018}, used a range of
global and local complexity measures to classify german secondary school texts for readability, while
\citet{shain2016}employed syntactic complexity measures using Dependency Locality Theory (DLT) to predict reading
times as a measure of cognitive load. Similarly, \citet{Feng2009} analyzed discourse level features related to
entity density in texts to assess the readability for adults with intellectual disabilities. % add a note that while

In contrast, most SLA studies on learner writing have focused primarily on linguistic complexity as a developmental
marker, using various measures to model language growth
\cite{Lu2010,Lu2011,Vyatkina2012,weiss2019,Iwashita2006,Wolfe1998, Ortega2003,NorrisOrtega2009}. However,
the majority of studies have focused on the domains of syntax and lexical complexity which has been criticized
as \"reductionist\" and too simplistic of an operationalization of linguistic complexity \cite{Butle2012}. In the
following sections below, I
will analyze the different domains of linguistic complexity used in this thesis.

%- Introduce major complexity metrics:
%    -Syntactic: sentence length, clause density, subordination, coordination, noun phrases, dependency distance
%    - Lexical: type-token ratio (TTR), MTLD, lexical sophistication
%-Theoretical basis and interpretation of each measure and limitations
%- Use of NLP tools in measuring complexity and automated tools in ICALL applications (CTAP)

\subsection{Syntactic Complexity}
%- General overview on types of measures
   % - Elaboration:  \citet{Butle2012}
    %- Variation : Diversity of forms and structures
%- introduce measures, especially ones I plan to use information on the calculations should be given in chapter 3 or
%in appendix?

%- both absolute and relative measures included
    %- mean clause length
    %- mean sent length
    %- clauses to sentences ratio (sentence complexity ratio)
    %- Complex NP per sentence
    %- NP per sentence
    %- coordination
    %- subordination
    %- adjective and adverb verb modifiers per VP?
    %- ADD - average dependency distance?

Syntactic complexity  is concerned with quantifying how elaborate and varied the grammatical structures used in a
    text are. A syntactically complex text often contains more structurally rich constructions, resulting in increased length, depth or diversity. Accordingly, measures of syntactic complexity are often divided into those capturing elaboration- referring to the use of
    longer or more embedded structures, and variation, which refers to the range and diversity of syntactic forms
    used.

\citet{Butle2012} further distinguishes syntactic complexity into two dimensions of scale: global and local
measures. Global
measures concern the macro-level structural organization of language or what is described as
\textit{"the size, breadth, width, or richness of the
learner's L2 system"} \cite{Butle2012}. These measures are typically operationalized at the sentence or clause level
and include metrics such as mean length of T-unit/sentence, clauses per T-unit/sentence or complex.They reflect
learner's ability to coordinate and embed clauses into hierarchically structured sentences.

In contrast, the local level addresses more fine-grained measures by analyzing
the internal structure at the phrase or clause level. Measures such as length of noun phrases,
modifiers per noun phrase, and Mean hierarchical distance (MHD) are used to analyze the internal structure and depth
at a more granular level. These local measures capture how syntactic elements are constructed and nested within
smaller constituents, such as within noun or verb phrases, providing insight into the learner's use of structurally
dense constructions.

For a historical perspective, the majority of syntactic complexity research has focused on absolute measures-those 
based on observable structural features of the langauge that can be quantified independently of the user. These 
measures at the global level are especially based on the T-unit
\footnote{terminable unit, a main clause plus its dependent clasuses
\cite{hunt1965}}, have a long legacy in SLA studies, beginning with oral data \cite{hunt1965} and later applied to
writtern learner data \cite{Ortega2003,Lu2011}. However, other researches implementing automated systems have
preferred to use the sentences \cite{Vyatkina2012,Lu2010}. \citet{Bardovi-Harlig1992} also argued against
artificially segmentind a learner's writing with T-units. As the learner's own knowledge on coordination within
their target language could be unintentionally discounted.

However, more recent approaches to syntactic complexity incorporate relative complexity, or the domain concerned
with processing demand. In this view, the complexity of a structure is not only determined by its length or depth
but by the cognitive effort required to comprehend or produce it. One such class of measures is derived from the 
psycholinguistic models such as Dependency Locality Theory (DLT) \cite{Gibson2000}, which proposes that greater linear distance 
between syntactically dependent elements increases processing load. Measures such as Mean Dependency Distance (MDD)\cite{Liu2008} and Mean Hierarchical Distance (MHD) \cite{Liu2017} have been used in studies to predict reading time and cognitive load \cite{shain2016, Feng2009}, as well as in SLA to model proficiency levels in Japanese learners\cite{Jiang2019,komori2019,Yang2023}. These relative complexity measures allow for a more nuanced view of syntactic complexity that account for both structural elaboration and difficulty in processing. % difficulty really doesn't play a role in my analysis though right? It is at least appealing that I am trying to keep borad measures.


\subsection{Lexical Complexity}
%- General overview on what is being measured and motivation behind it
%    - different types/domains text internal vs text external
%    -difference between lexical words and words
%- introduce measures MLTD, CTTR , also mention limitations?
%- lexical frequency measures different from syntactic complexity in that they don't quantify elaborateness (relative
%complexity measure)?
%- Lexical complexity measures are a contrast to criterial features which focus on feature presence linked to
%proficiency level

%    - Mean word length in syllables? (convert kanji to katakana)
%    - Max word length?
%    - MTLD (exclude punctuation)
%    - CTTR
%    - CTTR for each specific pos (adverbs, adjectives, nouns, verbs, )
%    - lexical density of different pos
%relative lexical complexity (language use)
%    - Lexical Sophistication (Average frequency in frequency data-base) \cite{kim2018}

Lexical complexity is commonly assessed through a variety of measures that capture both the sophistication and
diversity of a learner's vocabulary. Researchers have operationalized lexical complexity through two broad types of
measures: those that rely solely on the content of the learner's text, known as
text-internal measures, and those that incorporate external benchmarks such as frequency data from reference
corpora , referred to as and text-external measures\citep{Butle2012}.

Text internal measures primarily assess how diverse and structurally rich the vocabulary in a text is.
One of the
earliest
and
foundational measures is the Type Token Ratio (TTR) introduced by \citet{Templin1957}. TTR measures the porportion of
unique word types to the total number of tokens in a text. However, TTR is sensitive to text
length
\citep{koizumi2012}, typically decreasing as a text grows longer. To address this, alternative measures have been
proposed, such as the correct type token ratio (
CTTR)
\citep{Carroll1964}, which adjusts for length by dividing by the number of types by the square root of the
number of tokens multiplied by two. The Moving-Average Type-Token Ratio(MATTR)\citep{Covington2010} computes the
averate TTR over a series of fixed-length sliding windows across the text. Another widely
adopted measure is the Measure of Textual Lexical Diversity (MTLD)
\citep{McCarthy2010}. MTLD
calculates the average length of word
sequences that maintain a certain TTR value and is therefore less affected by text length, making it a suitable
measure to analyze learner texts which may vary significantly in length between proficiency levels.

Other internal measures
 of lexical complexity include
mean word length, which is sometimes used as a proxy for lexical sophistication
\footnote{ While correlation has been found between word length and sophistication (with the idea that longer words tend to be less frequent) there are many limitations in using this measure to represent lexical sophistication.}. Lexical density is another commonly used metric, which calculates the proportion
of content words-such as nouns, verbs, adjectives, and adverbs-in relation to the total word count. This is often
thought of as an index of information load, since texts with higher lexical density typically converse more
conceptual content relative to function words.

Text external measures draw on frequency data from reference corpora, usually compiled from native-speakers, to
assess lexical sophistication.
These
measures aim
to count not just how many different words are used, but how advanced or infrequency those words are in typical
language
use. One prominent approach is the Lexical Frequency Profile (LFP), introduced by \cite{Laufer1995}, which examines
the proportion of a learner's vocabulary falling into different frequency bands based on corpus-derived wordlists. A
learner whose writing features more low-frequency words is typically regarded as using more sophisticated vocabulary.
These measures offer a way to estimate the "rarity" or nativeness of a learner's lexical choices by benchmarking
them against large-scale data from fluent speakers.

By focusing on either the internal or external measures of a text, these two domains of lexical complexity
reveal different aspects of a learner's vocabulary. Text internal measures focus on diversity and structural
features, while text external features emphasize the specialized vocabulary.

\subsection{Morphological Complexity}
%    - absolute measures
%    - MCI-10 and MCI-5 for nouns verbs and adjectives
%    - inflection features, (past tense, passive, causitive)
%    - Derivation features (nominalization of type X per token using 可能-性 etc. also Adjectives with 的？)
%    - include information on Japanese as an agglutinative language?
Morphological complexity refers to the range, diversity, and structural elaboration or morphemes in a
learner's language production. Just like
Syntactic complexity, Morphological
complexity it can be approached from the dual perspectives of variation-diversity of forms used and
    elaboration-how morphologically dense or structurally intricate words are.

Morphological complexity has been less studied
compared to lexical and syntactic complexity in SLA research\citep(Bulte2012), particularly in English-focused
contexts, perhaps due to its sparse morphology compared to other languages. However, a growing body of research for
morphologically richer
languages such as Dutch \citep{vanderslik2019}, Italian
\citep{Brezina2019,dellorletta2011,Romano2017}, Korean \citep{Hwang2024},  Spanish \citep{Garcia2021, Malvern2004}, German
\citep{hancke2012-readability},
Russian \citep{reynolds2016-insights} and
French \cite{DeClercq2019,francois2012-ai}
has been
found.

Several quantitative measures have been proposed to operationalize morphological complexity, especially
from a diversity-based perspective. Inflectional Diversity
\citep{Malvern2004} simply tallies the
different inflection exponents a learner uses. The Normalized Mean Size of Paradign \citep{Xanthos2010} calculates
the average number of distinct forms per lemma, while also sampling tokens to mitigate text-length effects. Building
on these \cite{Brezina2019} proposed the Morphological Complexity Index (MCI) which compares variation in selected
parts of
speech-such as verb or noun
inflections-using
randomized resampling to control for sample size.

A more recent method, Korean Morpheme-based Richness Analysis (KMRA)
\citep{Hwang2024} was developed to account for the agglutinative nature of Korean. In this approach, all morphemes in
a text are categorized into three categories: function morphemes, content morphemes, and all-morphemes. Then, for
each group, two lexical diversity metrics are calculated-Moving Average Type-Token Ratio (MATTR)\citep{Covington2010} and MTLD\citep{McCarthy2010}-providing
a nuanced profile of morphological richness that distinguishes between grammatical and semantic contribution.

While the previous measures capture variation and breadth, local elaboration measures capture how intricately
morphological elements are structured within words-such as compound formation or layered
affiliation. For example \cite{Nishikawa2023} investigates the acquisition of Japanese case markers by l2 children
in naturalistic settings. Case particles such as が(ga),を(wo), and に(ni), ecode critical syntactic and semantic
information, and was demonstrated that they are acquired gradually in development. While not "elaborate" in form,
their useage reflects increasing morphological sensitivity and structural integration. Similar approaches have been
used in SLA studies to examine the proportion of tense, case, or derivational markers has been
examined
\citep{Verspoor2012,Guo2013, reynolds2016-insights,DeClercq2019}. Later emergence of these features in both child
and adult L3 development makes them useful indications of relative morphological complexity (processing difficulty)
\citep{Butle2012}.

%%%%%%%%%%%%%%% Some of the measures below may not be possible to include but I should still movtivate them and
%%%%%%%%%%%%%%%     explain what they measure
\subsection{Other Complexity Measures}

Phonetic complexity pertains to the intricacy of speech sounds and their combinations within a language, including
features such as syllable structure, consonant clusters, intonation, and articulation\citep{Maddieson2009}.
These measures have mostly been used to compare phonological features across different languages. While not
widely applied in SLA contexts-due to data being predominately in the written form, phonetic complexity measures have
been
used to measure development in
children's
phonological systems across time\citep{StoelGammon2010}.

Semantic complexity refers to the richness and depth of meaning express through vocabulary and conceptual
relationships in a text. Although not explicitly included in the framework proposed by \cite{Butle2012}, semantic
complexity can be associated with factors such as abstractness, and relatedness between words. Some studies have
linked indices of semantic sophistication to writing quality \citep{Kyle2018}. While others have used Latent Semantic
Analysis (LSA) or word embeddings, showing that higher semantic similarity between sentences is linked to better
writing
quality
\citep{Crossley2011, Briscoe2010}

Discourse complexity refers to the degree of structural, semantic, and logical elaboration across larger units of
text, such as multiple sentences, paragraphs, or entire texts. In \cite{Butle2012}, discourse complexity is
defined in
terms of
interactional moves and roles taken by participants in a conversation, focusing on the domain of
oral
interaction and disregarding written language. One way to analyze discourse in written texts is through the measures of
coherence and cohesion.
COH-METRIX \cite{Graesser2004} is an automated tool that analyzes the cohesion in written texts. It calculates over
100 indices across lexical, syntactic and discourse levels, with a primary focus on how well ideas in a text are
explicitly connected. Some example measures include the ratio of personal pronouns to nouns, which effects
referential cohesion, and the frequency of connectives
such as \textit{because}, \textit{however}, and \textit{after} which illustrate relationships between ideas in a text.

% motivate in methodlolgy section the few complexity domains I will not implement

\section{Linguistic Complexity in Second Language Acquisition (SLA)}

%Brief intro here motivating the uses of these complexity measures - the relative measures are usually used in
%relation to readability studies and absolute have been studied for SLA as indices for development.

Linguistic complexity has been a focus in both applied linguistics and language education, particularly in
understanding the developmental trajectory of second language (L2) learners. Complexity measures provide insights
into how language use changes over time, offering a quantitative view which researchers and educators can use to
evaluate progress in language proficiency within the different linguistic domains. Learner corpora have proven
extremely valuable to
researchers with the ability to map indices to different stages of development (or proficiency levels).

Given the two domains described above, relative complexity measures-which have traditionally compared text difficulty
across corpora or learner groups-have been widely used in readability studies
\citep{shain2016, Feng2009, dellorletta2011, francois2012-ai,Berendes2018}. These metrics help estimate how accessible a
given
text is to a
reader
based on structural and lexical characteristics. They can also be used in evaluating simplified text in the task of
text simplification. On the other hand, absolute complexity measures aim to assess the linguistic features
of a learner's output without direct comparison to other texts. In SLA research, these absolute indices are
increasingly recognized for their potential to serve as developmental benchmarks, illustrating the evolution of the
learner's language abilities over time.

\subsection{Linguistic Complexity as an Index of Language Development}
%-Syntactic complexity and lexical measures as indices for proficiency levels of other languages (besides Japanese -
%which will have its own section). (Language
%Developmental Trajectory)
    %-Discuss measures chosen across studies
    %- What has been said about choosing measures
    %-Developmental Trajectory - what have studies found?
%-role of learner corpora in operationalizing and tracking complexity.

%%%%%%%%%%%include:
%-task effect
%summarize Findings in other languages and domains
Linguistic complexity has been widely recognized as a reliable indicator of language proficiency and development,
particularly in the lexicographical and syntactic domains....

In studies of English as a second language, a variety of syntactic features have been shown to index developmental
progression. For instance, the average number of words per T-unit has consistently emerged as a robust measure
across multiple studies \citep{Ortega2003,Wolfe1998,Lu2011, Lu2010, Iwashita2006}. Similarly, alternative unit-based
metrics such as sentence length-rather than clause-based units-have also shown strong discrimination between
proficiency levels \citep{Ortega2003, Lu2011}.

More fine-grained clausal complexity measures, such as average clause length and clauses per sentence further
contribute to this profiling. They have shown reliable patterns of increased syntactic embedding with learner
development across proficiency levels, suggesting a trajectory toward more hierarchically structured sentences \citep{Ortega2003, Lu2011}.

Due to the limitations in design of learner corpora (most being cross-sectional instead of longitudinal) need more
longitudinal studies to verify the above findings. In her study of German L2 learners, \cite{Vyatkina2012} found
that both general and clausal complexity measures increased over time, correlating with the learners' developmental
progression. Notably, the study revealed a shift in coordination and subordination: as learners become more
proficient, coodinating structures decreased, while subordinating constructions became more prevalent. This suggests
a qualitative shift in syntactic structure, where learners begin to use more embedded structures to express more
nuanced
meanings.


\subsection{Complexity Research in Japanese}
%-Any contradictory/surprising findings to English
%-Studies on Learner Japanese
Despite being classified as a Less Commonly Taught Language (LCTL) by the National Council of Less Commonly Taught
Languages \cite{ncolctl2025}, Japanese has seen a steady rise in learner enrollment since the early 2000s. Given its
typological distinctiveness -particularly in terms of agglutinative morphology, SOV word order, and elaborate
honorific and polite registers, Japanese offers a valuable opportunity to evaluate the generalizability of
complexity measures developed in research on Indo-European languages.

\cite{Iwashita2006} explored syntactic complexity in oral narratives from learners at different proficiency levels (
three vs. four semesters of instruction). The study found increases in global length-based measures such as sentence
length and clause language, but no significant differences for subordination or coordination, suggesting that these
features could possibly be acquired earlier, plateau earlier, or emerge later compared to English.

More recent studies have begun incorporating a braoder range of complexity measures. \cite{komori2019} evaluated
Morphological Derivation Density (MDD) and Morphological Head Density (MHD) across beginner and intermediate L2
Japanese learners, alongside a native speaker control group. The results indicated that MHD, which focuses on
syntactic head positions, was more sensitive to distinguishing between proficency levels, suggesting its usefulness
as an index of morphosyntactic development. However, this study was limited in scope, comparing only two learner
levels- 3 semesters vs. 4 semesters of study.

In a longitudinal study, \cite{Yang2023} tracked syntactic development via MDD and MHD among native chinese-speaking
learners of Japanese. The study found that while MDD remained relatively stable over time, MHD increased, suggesting
that learners produced more hierarchically complex sentence structures as their proficiency developed. These
findsing support the notion that hierarchical structuring may be a more sensitive indicator for syntactic development
in Japanese than linear dependency distance(as in MDD), aligning with earlier claims about the limits of
clause-based measures in typologically distinct languages like Japanese \citep{Iwashita2006}. These measures have
not been evaluated together to validate which measures may be most appropriate as indices for development. The
complexity analysis also have purely focused on the syntax domain of linguistics, with
analysis of other domains neglected.

\subsection{Applications in SLA Research, Readabiliity, ICALL}
- CTAP
- Readability Assessment applications
- ICALL applications


\section{Criterial Features}

While linguistic complexity focuses on the structural elaborateness and variation in learner language, Criterial
features as introduced
    by \citet{Hawkins_Buttery_2010} focus on the
    presence or
    absence of specific linguistic forms in a learner's
    productions at different
    stages of development. The key intutition is that certain morphosyntactic structures, lexical items, or
grammatical patterns reliably emerge (or disappear) at specific proficency levels. These features can include the use of certain grammar structures, lexical items, or morphosyntactic
forms shown to correlate to specific proficiency levels, thus serving as qualitative indicators of L2 proficiency.

In contrast to Complexity measures, which are continuous metric of
elaboration
or variation in a text, criterial features tend to be discrete markers of development. That is, instead of measuring
how much or how varied are concerned with whether a specific form is used accurately and/or frequently enough to
indicate a given proficiency level.

Criterial features may be analyzed across two dimensions, frequency, which captures the emergence and increasing use
of a form across proficiency and accuracy, which assess the learner's ability to use a form correctly (in comparison
to native speaker's uses) of the forms across levels. An example of this would be the use of passive
constructions may increase across proficiency levels but only consistent correct usage may appear from upper
intermediate levels and onwards. These dual perspectives are useful for distinguishing between emergence and mastery. For instance, learners may
begin to produce relative clauses at lower levels, but only achieve an accurate usage at the upper-intermediate
levels.

\citet{Hawkins_Buttery_2010} further distinguish between positive and negative criterial features.
Positive features appear at a certain level, but not before and therefore inform the emergence of a
feature.
Negative features on the other hand, are features which are absent or persistent errors at the lower levels but
disappear with development as learners master a form.

Their corpus-based analysis also revealed L1 effects. For example, learners whose
first language contained an article system (e.g. French, German, or Spanish), had a lower rates of missing articles
compared to those whose L1 lacks articles. This supports the view that not all criterial features are universal, and
specific developmental profiles may be needed. \citet{salamoura2010} also observed the influence of L1 weakening at
high proficiency levels.
\citet{salamoura2010} also working within the English Profile Programme, analyzed learner texts in the Cambridge
Learner Corpus (
CLC) to identify features distinguishing between CEFR proficiency levels.
Their findings support those of
\citet{Hawkins_Buttery_2010,diez-bedmar2015,Kim2021}, observing that learner errors decrease and L1 influence weakens at
the higher
proficiency levels.

\citet{diez-bedmar2015} -spanish ESL learners article useage (not necessarily aligned with CEFR?)
\citet{Kim2021} - Korean EFL Learners - feature use increased with proficiency level - although overall frequencies
were still low - feature misalignments?? , task effect?


%%Transition paragraph??
%transition paragraph to go into JLPT framework....
The majority of work in this domain has focused exclusively on English. However, \citet{akef2025} used criterial
features in modeling writings from heritage learners of Portuguese, frequency of some of the grammar forms was shown
to be a significant variable in classifying proficiency level. Aligning with findings from other studies an effect
of the learners' spoken language
\footnote{spoken language, refers to the main/majority language spoken in the country/area where the learner resides}
was found.

The above findings not only support the value of criterial features as developmental indicators, but also
demonstrate the need to systemize them across proficiency levels. This has motivated profiling efforts \cite{Saville2010}
aligned
with
the standardized frameworks such as the Common European Framework of Reference (CEFR).



\subsection{Proficiency Assessment Frameworks}
%- Overview of CEFR levels
%- The structure and goals of JLPT (grammar/lexicon targets at each level)
%- Historical development of JLPT and including of CEFER-style descriptors from 2012.
%- limitations of JLPT, or critiques on standardized tests.
%Not sure if this warrants it's own section. Describe proficency assesment measures CEFER, etc. also introduce JLPT
%Can-do statements- forms teste

%- JLPT vocabulary lists and grammar lists

%- Examples of criteria features in English (and possible other languages?) also describe this in relevance to Japanese
%    - English Grammar Profile
%    -CEFR-J Grammar Profile
%    - should there be specialized profiles based on a learner's native language?
%- automated extraction - POLKE

To contextualize criterial features within standardized evaluation it is essential to consider the role of
proficiency assessment frameworks, which provide structured levels for describing and assessing a learner's language
ability. For many european languages the Common European Framework of Reference for languages (CEFR) is the most
widely adopted framework, which offers six proficiency levels. Illustrative
descriptors are given for each level in the framework to illustrate what learners can achieve in communicative
competence at
each
level \citep{CEFR2020}. As can be seen in Table~\ref{tab:cefr-descriptors}, these guidelines are intentionally
vague to be easily
adopted
into other
languages.

\begin{table}
\centering
\begin{tabular}{cc}
\hline \textbf{CEFR Level} & \textbf{Illustrative Descriptor} \\ \hline
C2 & Can handle difficult and hostile questioning \\
B1  & Can narrate a story \\
A2 & Can give short, basic descriptions of events and activities \\
\hline
\end{tabular}
\caption{\label{tab:cefr-descriptors} Examples of CEFR Illustrative examples}
\end{table}

Efforts such
as the English Profile Programme \citep{Saville2010}, have emerged to support the development detailed reference level
descriptions
of the learners at each CEFR level, one of the subprojects- English Grammar Profile (EGP) \citep{okeeffe2017}
cataloged over 1,200 grammar forms of learner language across each of the CEFR proficiency levels.
\citet{tono2018,ishii2016} have
also adapted a
finer-grained
proficiency scale
of criterial features
geared
towards native Japanese learners of English called the CEFR-J.

%POLKE project add later if I can get a citation.


\subsubsection{Japanese Proficiency Assessment Frameworks}
While CEFR was designed to be applied to any language - the Japanese Language Proficiency Test(JPLT) remains the
most established framework for assessing Japanese as a second language. Established in 1984 \citet{JLPTinfo}, the
test had 4
proficency bands
designed to evaluate the proficiency of Japanese learners. Learners are not given a proficency level but must choose the
proficiency band of the test they would like to pass. The test was revised in 2010 to add an additional level and to
better balance the difficulty across levels. Learners are evaluated for the following domains: reading, listening,
vocabulary, and grammar. In contrast to the CEFR framework which uses general statements to describe learner's
ability at a certain proficency level,
Can-Do statements describing the
capabilities of
learners at
each
JLPT band were recently added in 2012. While official evaluation criteria are not released by the Japan Foundation,
unofficial lists of grammar, vocabulary and Kanji are have been compiled from the official practice workbooks
published by the Japan Foundation.

One criticism of the JLPT test is the lack of assessment for a learner's
speaking and writing skills \citet{JLPTcriticism}.


\subsubsection{Automatic Proficency Assesment}
- motivation - describe why automated is needed
- Overview of existing systems and which features they use for proficency classification.
- Describe challenges
    - Learner errors impacting feature extraction (decreases robustness)
    -Inconsistent tagging/parsing in morphologically rich or low-resource languages
        -try to find something for Japanese to support some inconsistentices I found when tokenizing sample text
- Describe the aboves' relevance to Japanese, and to this study

These features metnioned above used for assigning proficency level texts ...

\section{Explainable Boosting Machines}
% Overview of what they do and how they work to tie them together with my methodology.Include applications of EBMS in
% SLA and/or compare and contrast other machine learning methods.
- EBMS in SLA
\citep{akef2025}